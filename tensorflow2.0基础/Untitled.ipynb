{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 50, loss: 0.086001, W: 0.197634, b: 1.181646\n",
      "step: 100, loss: 0.084964, W: 0.200816, b: 1.159088\n",
      "step: 150, loss: 0.084045, W: 0.203810, b: 1.137859\n",
      "step: 200, loss: 0.083231, W: 0.206628, b: 1.117881\n",
      "step: 250, loss: 0.082510, W: 0.209280, b: 1.099081\n",
      "step: 300, loss: 0.081872, W: 0.211775, b: 1.081388\n",
      "step: 350, loss: 0.081306, W: 0.214124, b: 1.064738\n",
      "step: 400, loss: 0.080806, W: 0.216334, b: 1.049069\n",
      "step: 450, loss: 0.080362, W: 0.218414, b: 1.034323\n",
      "step: 500, loss: 0.079970, W: 0.220371, b: 1.020446\n",
      "step: 550, loss: 0.079622, W: 0.222213, b: 1.007387\n",
      "step: 600, loss: 0.079314, W: 0.223947, b: 0.995097\n",
      "step: 650, loss: 0.079041, W: 0.225578, b: 0.983531\n",
      "step: 700, loss: 0.078800, W: 0.227114, b: 0.972646\n",
      "step: 750, loss: 0.078586, W: 0.228558, b: 0.962403\n",
      "step: 800, loss: 0.078396, W: 0.229918, b: 0.952763\n",
      "step: 850, loss: 0.078228, W: 0.231198, b: 0.943691\n",
      "step: 900, loss: 0.078080, W: 0.232402, b: 0.935154\n",
      "step: 950, loss: 0.077948, W: 0.233535, b: 0.927120\n",
      "step: 1000, loss: 0.077831, W: 0.234602, b: 0.919560\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "rng = np.random\n",
    "# 参数\n",
    "learning_rate = 0.01\n",
    "training_steps = 1000\n",
    "display_step = 50\n",
    "# 训练数据\n",
    "X = np.array([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,\n",
    "              7.042,10.791,5.313,7.997,5.654,9.27,3.1])\n",
    "Y = np.array([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,\n",
    "              2.827,3.465,1.65,2.904,2.42,2.94,1.3])\n",
    "n_samples = X.shape[0]\n",
    "\n",
    "# 随机初始化权重，偏置项\n",
    "W = tf.Variable(rng.randn(), name=\"weight\")\n",
    "b = tf.Variable(rng.randn(), name=\"bias\")\n",
    "\n",
    "# 线性回归(Wx+b)\n",
    "def linear_regression(x):\n",
    "    return W * x + b\n",
    "\n",
    "# 均方差\n",
    "def mean_square(y_pred, y_true):\n",
    "    return tf.reduce_sum(tf.pow(y_pred - y_true, 2)) / (2 * n_samples)\n",
    "\n",
    "# 随机梯度下降优化器\n",
    "optimizer = tf.optimizers.SGD(learning_rate)\n",
    "\n",
    "# 优化过程\n",
    "def run_optimization():\n",
    "    # 将计算封装在GradientTape中以实现自动微分\n",
    "    with tf.GradientTape() as g:\n",
    "        pred = linear_regression(X)\n",
    "        loss = mean_square(pred, Y)\n",
    "\n",
    "    # 计算梯度\n",
    "    gradients = g.gradient(loss, [W, b])\n",
    "\n",
    "    # 按gradients更新 W 和 b\n",
    "    optimizer.apply_gradients(zip(gradients, [W, b]))\n",
    "# 针对给定训练步骤数开始训练\n",
    "for step in range(1, training_steps + 1):\n",
    "    # 运行优化以更新W和b值\n",
    "    run_optimization()\n",
    "\n",
    "    if step % display_step == 0:\n",
    "        pred = linear_regression(X)\n",
    "        loss = mean_square(pred, Y)\n",
    "        print(\"step: %i, loss: %f, W: %f, b: %f\" % (step, loss, W.numpy(), b.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
