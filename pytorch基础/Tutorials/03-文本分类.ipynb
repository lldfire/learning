{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchtext\n",
    "import pandas as pd\n",
    "\n",
    "from torch import optim\n",
    "from torchtext.datasets import text_classification\n",
    "\n",
    "\n",
    "NGRAMS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torchtext.datasets.text_classification.AG_NEWS(*args, **kwargs)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchtext.datasets.AG_NEWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数据读取\n",
    "数据集需要下载，然而因为防火墙的原因无法下载Google云上的数据集，因此手动下载了数据集，并改写了数据集读取的方法，使其不需要下载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 此方法需要下载数据集，事实上数据集已经下载完成，因此通过改写下面cell的方法读取数据集\n",
    "# train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n",
    "#     root='./data', ngrams=NGRAMS, vocab=None)\n",
    "# BATCH_SIZE = 16\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import torch\n",
    "import io\n",
    "from torchtext.utils import download_from_url, extract_archive, unicode_csv_reader\n",
    "from torchtext.data.utils import ngrams_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.vocab import Vocab\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def _csv_iterator(data_path, ngrams, yield_cls=False):\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "    with io.open(data_path, encoding=\"utf8\") as f:\n",
    "        reader = unicode_csv_reader(f)\n",
    "        for row in reader:\n",
    "            tokens = ' '.join(row[1:])\n",
    "            tokens = tokenizer(tokens)\n",
    "            if yield_cls:\n",
    "                yield int(row[0]) - 1, ngrams_iterator(tokens, ngrams)\n",
    "            else:\n",
    "                yield ngrams_iterator(tokens, ngrams)\n",
    "\n",
    "\n",
    "def _create_data_from_iterator(vocab, iterator, include_unk):\n",
    "    data = []\n",
    "    labels = []\n",
    "    with tqdm(unit_scale=0, unit='lines') as t:\n",
    "        for cls, tokens in iterator:\n",
    "            if include_unk:\n",
    "                tokens = torch.tensor([vocab[token] for token in tokens])\n",
    "            else:\n",
    "                token_ids = list(filter(lambda x: x is not Vocab.UNK, [vocab[token]\n",
    "                                        for token in tokens]))\n",
    "                tokens = torch.tensor(token_ids)\n",
    "            if len(tokens) == 0:\n",
    "                logging.info('Row contains no tokens.')\n",
    "            data.append((cls, tokens))\n",
    "            labels.append(cls)\n",
    "            t.update(1)\n",
    "    return data, set(labels)\n",
    "\n",
    "\n",
    "class TextClassificationDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Defines an abstract text classification datasets.\n",
    "       Currently, we only support the following datasets:\n",
    "\n",
    "             - AG_NEWS\n",
    "             - SogouNews\n",
    "             - DBpedia\n",
    "             - YelpReviewPolarity\n",
    "             - YelpReviewFull\n",
    "             - YahooAnswers\n",
    "             - AmazonReviewPolarity\n",
    "             - AmazonReviewFull\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab, data, labels):\n",
    "        \"\"\"Initiate text-classification dataset.\n",
    "\n",
    "        Arguments:\n",
    "            vocab: Vocabulary object used for dataset.\n",
    "            data: a list of label/tokens tuple. tokens are a tensor after\n",
    "                numericalizing the string tokens. label is an integer.\n",
    "                [(label1, tokens1), (label2, tokens2), (label2, tokens3)]\n",
    "            label: a set of the labels.\n",
    "                {label1, label2}\n",
    "\n",
    "        Examples:\n",
    "            See the examples in examples/text_classification/\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        super(TextClassificationDataset, self).__init__()\n",
    "        self._data = data\n",
    "        self._labels = labels\n",
    "        self._vocab = vocab\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self._data[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for x in self._data:\n",
    "            yield x\n",
    "\n",
    "    def get_labels(self):\n",
    "        return self._labels\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return self._vocab\n",
    "\n",
    "\n",
    "\n",
    "def _setup_datasets(dataset_name, root='.data', ngrams=1, vocab=None, include_unk=False):\n",
    "    # dataset_tar = download_from_url(URLS[dataset_name], root=root)\n",
    "    extracted_files = extract_archive(root)\n",
    "\n",
    "    for fname in extracted_files:\n",
    "        if fname.endswith('train.csv'):\n",
    "            train_csv_path = fname\n",
    "        if fname.endswith('test.csv'):\n",
    "            test_csv_path = fname\n",
    "\n",
    "    if vocab is None:\n",
    "        logging.info('Building Vocab based on {}'.format(train_csv_path))\n",
    "        vocab = build_vocab_from_iterator(_csv_iterator(train_csv_path, ngrams))\n",
    "    else:\n",
    "        if not isinstance(vocab, Vocab):\n",
    "            raise TypeError(\"Passed vocabulary is not of type Vocab\")\n",
    "    logging.info('Vocab has {} entries'.format(len(vocab)))\n",
    "    logging.info('Creating training data')\n",
    "    train_data, train_labels = _create_data_from_iterator(\n",
    "        vocab, _csv_iterator(train_csv_path, ngrams, yield_cls=True), include_unk)\n",
    "    logging.info('Creating testing data')\n",
    "    test_data, test_labels = _create_data_from_iterator(\n",
    "        vocab, _csv_iterator(test_csv_path, ngrams, yield_cls=True), include_unk)\n",
    "    if len(train_labels ^ test_labels) > 0:\n",
    "        raise ValueError(\"Training and test labels don't match\")\n",
    "    return (TextClassificationDataset(vocab, train_data, train_labels),\n",
    "            TextClassificationDataset(vocab, test_data, test_labels))\n",
    "\n",
    "\n",
    "def AG_NEWS(*args, **kwargs):\n",
    "    \"\"\" Defines AG_NEWS datasets.\n",
    "        The labels includes:\n",
    "            - 0 : World\n",
    "            - 1 : Sports\n",
    "            - 2 : Business\n",
    "            - 3 : Sci/Tech\n",
    "\n",
    "    Create supervised learning dataset: AG_NEWS\n",
    "\n",
    "    Separately returns the training and test dataset\n",
    "\n",
    "    Arguments:\n",
    "        root: Directory where the datasets are saved. Default: \".data\"\n",
    "        ngrams: a contiguous sequence of n items from s string text.\n",
    "            Default: 1\n",
    "        vocab: Vocabulary used for dataset. If None, it will generate a new\n",
    "            vocabulary based on the train data set.\n",
    "        include_unk: include unknown token in the data (Default: False)\n",
    "\n",
    "    Examples:\n",
    "        >>> train_dataset, test_dataset = torchtext.datasets.AG_NEWS(ngrams=3)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    return _setup_datasets(*((\"AG_NEWS\",) + args), **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "DATASETS = {\n",
    "    'AG_NEWS': AG_NEWS,\n",
    "}\n",
    "\n",
    "\n",
    "LABELS = {\n",
    "    'AG_NEWS': {\n",
    "        0: 'World',\n",
    "        1: 'Sports',\n",
    "        2: 'Business',\n",
    "        3: 'Sci/Tech'\n",
    "    }, \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看了解数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120000lines [00:07, 16939.40lines/s]\n",
      "120000lines [00:13, 9015.48lines/s]\n",
      "7600lines [00:00, 9396.13lines/s]\n"
     ]
    }
   ],
   "source": [
    "# 数据读取完成\n",
    "train_dataset, test_dataset = DATASETS['AG_NEWS'](\n",
    "    root='./data/ag_news_csv.tar.gz', ngrams=NGRAMS, vocab=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 构建模型\n",
    "文本嵌入模型，将文本通过词语查询表转化为索引，再通过嵌入词袋层转化为线性层并输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class TextSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange= .5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "        \n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数设置\n",
    "BATCH_SIZE = 16    # 每轮训练的数据量\n",
    "VOCAB_SIZE = len(train_dataset.get_vocab())   # 向量大小即嵌入层的大小\n",
    "EMBED_DIM = 32    # 每个嵌入层大小\n",
    "NUN_CLASS = len(train_dataset.get_labels())   # 类别数量\n",
    "model = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUN_CLASS).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 生成批量处理的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch):\n",
    "    label = torch.tensor([entry[0] for entry in batch])\n",
    "    text = [entry[1] for entry in batch]\n",
    "    offsets = [0] + [len(entry) for entry in text]\n",
    "    # torch.Tensor.cumsum returns the cumulative sum\n",
    "    # of elements in the dimension dim.\n",
    "    # torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0)\n",
    "\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text = torch.cat(text)\n",
    "    return text, offsets, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(sub_train_):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    \n",
    "    data = DataLoader(sub_train_, batch_size=BATCH_SIZE,shuffle=True,\n",
    "                     collate_fn=generate_batch)\n",
    "    for i, (text, offsets, cla) in enumerate(data):\n",
    "        optimizer.zero_grad()\n",
    "        text, offsets, cla = text.to(device), offsets.to(device), cla.to(device)\n",
    "        output = model(text, offsets)\n",
    "        \n",
    "        loss = criterion(output, cla)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += (output.argmax(1) == cla).sum().item()\n",
    "    scheduler.step()\n",
    "    return train_loss / len(sub_train_), train_acc / len(sub_train_)\n",
    "\n",
    "\n",
    "def test(data_):\n",
    "    loss = 0 \n",
    "    acc = 0\n",
    "    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
    "    \n",
    "    for text, offsets, cla in data:\n",
    "        text, offsets, cla = text.to(device), offsets.to(device), cla.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(text, offsets)\n",
    "            \n",
    "            loss = criterion(output, cla)\n",
    "            loss += loss.item()\n",
    "            acc += (output.argmax(1) == cla).sum().item()\n",
    "    return loss / len(data_), acc / len(data_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 分割数据集并训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "\n",
    "N_EPOCHS = 5\n",
    "min_valid_loss = float('inf')\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)  # 损失函数\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=4.0)   # 优化器\n",
    "# 调度器\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "\n",
    "\n",
    "train_len = int(len(train_dataset) * 0.95)\n",
    "sub_train_, sub_valid_ = random_split(\n",
    "    train_dataset, [train_len, len(train_dataset) - train_len])\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train_func(sub_train_)\n",
    "    valid_loss, valid_acc = test(sub_valid_)\n",
    "    \n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs // 60\n",
    "    secs = secs % 60\n",
    "    \n",
    "    print('Epoch: %d' % (epoch + 1), '| time in %d minutes, %d seconds' % (mins, secs))\n",
    "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "96 % 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成数据集\n",
    "from torchtext.data import TabularDataset\n",
    "# Field 类处理确定如何处理数据并将其转化为数字\n",
    "from torchtext.data import Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用空白标记分割词语，并将字母小写\n",
    "tokenize = lambda x: x.split()\n",
    "TEXT = Field(sequential=True, tokenize=tokenize, lower=True)\n",
    "\n",
    "# 处理标签\n",
    "LABEL = Field(sequential=False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 加载训练集和验证集数据\n",
    "\n",
    "tv_datafields = [\n",
    "    ('id', None), ('comment_text', TEXT), ('toxic', LABEL),\n",
    "    ('severe_toxic', LABEL), ('obscene', LABEL), ('threat', LABEL),\n",
    "    ('insult', LABEL), ('identity_hate', LABEL), \n",
    "]\n",
    "\n",
    "trn, vld = TabularDataset.splits(\n",
    "    path='./.data',\n",
    "    train='train.csv', validation='valid.csv',\n",
    "    format='csv', skip_header=True, fields=tv_datafields\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_datafields = [\n",
    "    ('id', None),\n",
    "    ('comment_text', TEXT)\n",
    "]\n",
    "tst = TabularDataset(\n",
    "    path='./.data/test.csv',\n",
    "    format='csv',\n",
    "    skip_header=True,\n",
    "    fields=tst_datafields\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 78),\n",
       " ('to', 41),\n",
       " ('you', 33),\n",
       " ('of', 30),\n",
       " ('and', 26),\n",
       " ('a', 26),\n",
       " ('is', 24),\n",
       " ('that', 22),\n",
       " ('i', 20),\n",
       " ('if', 19)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Iterator, BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    }
   ],
   "source": [
    "train_iter, valid_iter = BucketIterator.splits(\n",
    "    (trn, vld),\n",
    "    batch_sizes=(64, 64),\n",
    "    device=-1,\n",
    "    sort_key=lambda x: len(x.comment_text),\n",
    "    sort_within_batch=False,\n",
    "    repeat=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.data.batch.Batch of size 25]\n",
       "\t[.comment_text]:[torch.LongTensor of size 494x25]\n",
       "\t[.toxic]:[torch.LongTensor of size 25]\n",
       "\t[.severe_toxic]:[torch.LongTensor of size 25]\n",
       "\t[.obscene]:[torch.LongTensor of size 25]\n",
       "\t[.threat]:[torch.LongTensor of size 25]\n",
       "\t[.insult]:[torch.LongTensor of size 25]\n",
       "\t[.identity_hate]:[torch.LongTensor of size 25]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_iter.__iter__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    }
   ],
   "source": [
    "test_iter = Iterator(tst, batch_size=64, device=-1, sort=False, sort_within_batch=False, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchWrapper:\n",
    "    def __init__(self, dl, x_var, y_vars):\n",
    "        self.dl, self.x_var, self.y_vars = dl, x_var, y_vars\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for batch in self.dl:\n",
    "            x = getattr(batch, self.x_var)\n",
    "            \n",
    "            if self.y_vars is not None:\n",
    "                y = torch.cat([getattr(batch, feat).unsqueeze(1) for feat in self.y_vars], dim=1).float()\n",
    "            else:\n",
    "                y = torch.zeros((1))\n",
    "            yield (x, y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = BatchWrapper(train_iter, \"comment_text\", [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"])\n",
    "valid_dl = BatchWrapper(valid_iter, \"comment_text\", [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"])\n",
    "test_dl = BatchWrapper(test_iter, \"comment_text\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(train_dl.__iter__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBiLSTMB(nn.Module):\n",
    "    def __init__(self, hidden_dim, emb_dim=300,\n",
    "                spatial_dropout=0.05, recurrent_dropout=0.1, num_linear=1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(len(TEXT.vocab), emb_dim)\n",
    "        self.encoder = nn.LSTM(emb_dim, hidden_dim, num_layers=1, dropout=recurrent_dropout)\n",
    "        self.linear_layers = []\n",
    "        for _ in range(num_linear - 1):\n",
    "            self.linear_layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "        self.linear_layers = nn.ModuleList(self.linear_layers)\n",
    "        self.predictor = nn.Linear(hidden_dim, 6)\n",
    "    \n",
    "    def forward(self, seq):\n",
    "        hdn, _ = self.encoder(self.embedding(seq))\n",
    "        feature = hdn[-1, :, :]\n",
    "        for layer in self.linear_layers:\n",
    "            feature = layer(feature)\n",
    "        preds = self.predictor(feature)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_sz = 100\n",
    "nh = 500\n",
    "nl = 3\n",
    "model = SimpleBiLSTMB(nh, emb_dim=em_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm     # 进度条库\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam(model.parameters(), lr=1e-2)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.17s/it]\n",
      "  0%|                                                                        | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 3.0838, Validatioon Loss: 2.7874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.14s/it]\n",
      "  0%|                                                                        | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss: 3.9316, Validatioon Loss: 2.1592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.77s/it]\n",
      "  0%|                                                                        | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss: 2.9060, Validatioon Loss: 2.1450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 1/1 [00:08<00:00,  8.79s/it]\n",
      "  0%|                                                                        | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss: 3.1643, Validatioon Loss: 2.2619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 1/1 [00:09<00:00,  9.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Training Loss: 3.1872, Validatioon Loss: 2.3949\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    running_loss = 0\n",
    "    running_corrects = 0\n",
    "    model.train()\n",
    "    for x, y in tqdm.tqdm(train_dl):\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        preds = model(x)\n",
    "        loss = loss_func(preds, y)\n",
    "        loss.backward()\n",
    "        \n",
    "        opt.step()\n",
    "        \n",
    "        running_loss += loss.item() * x.size(0)\n",
    "        \n",
    "    epoch_loss = running_loss / len(trn)\n",
    "    \n",
    "    val_loss = .0\n",
    "    model.eval()\n",
    "    for x, y in valid_dl:\n",
    "        preds = model(x)\n",
    "        loss = loss_func(preds, y)\n",
    "        val_loss += loss.item() * x.size(0)\n",
    "    val_loss /= len(vld)\n",
    "    print(f'Epoch: {epoch}, Training Loss: {epoch_loss:.4f}, Validatioon Loss: {val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 334.34it/s]\n"
     ]
    }
   ],
   "source": [
    "test_preds = []\n",
    "for x, y in tqdm.tqdm(test_dl):\n",
    "    preds = preds.data.numpy()\n",
    "    preds = 1 / (1 + np.exp(-preds))\n",
    "    test_preds.append(preds)\n",
    "test_preds = np.hstack(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
